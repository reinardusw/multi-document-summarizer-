{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MENYIAPKAN ENVIRONMENT TUGAS AKHIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import getopt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from GetOldTweets import got3 as got\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MENGUMPULKAN TWEETS DENGAN QUERY TERTENTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mendapatkan query yang diinginkan\n",
    "def getTweet(argv):\n",
    "    if len(argv) == 0:\n",
    "        print('You must pass some parameters. Use \\\"-h\\\" to help.')\n",
    "        return\n",
    "\n",
    "    if len(argv) == 1 and argv[0] == '-h':\n",
    "        f = open('exporter_help_text.txt', 'r')\n",
    "        print(f.read())\n",
    "        f.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv, \"\", (\n",
    "        \"username=\", \"near=\", \"within=\", \"since=\", \"until=\", \"querysearch=\", \"toptweets\", \"maxtweets=\", \"output=\", \"lang=\"))\n",
    "\n",
    "        tweetCriteria = got.manager.TweetCriteria()\n",
    "        outputFileName = \"TweetsTA.csv\"\n",
    "\n",
    "        for opt, arg in opts:\n",
    "            if opt == '--username':\n",
    "                tweetCriteria.username = arg\n",
    "\n",
    "            elif opt == '--since':\n",
    "                tweetCriteria.since = arg\n",
    "\n",
    "            elif opt == '--until':\n",
    "                tweetCriteria.until = arg\n",
    "\n",
    "            elif opt == '--querysearch':\n",
    "                tweetCriteria.querySearch = arg\n",
    "\n",
    "            elif opt == '--toptweets':\n",
    "                tweetCriteria.topTweets = True\n",
    "\n",
    "            elif opt == '--maxtweets':\n",
    "                tweetCriteria.maxTweets = int(arg)\n",
    "\n",
    "            elif opt == '--near':\n",
    "                tweetCriteria.near = '\"' + arg + '\"'\n",
    "\n",
    "            elif opt == '--within':\n",
    "                tweetCriteria.within = '\"' + arg + '\"'\n",
    "\n",
    "            elif opt == '--output':\n",
    "                outputFileName = arg\n",
    "\n",
    "            elif opt == '--lang':\n",
    "                tweetCriteria.lang = arg\n",
    "\n",
    "        #membuat file csv\n",
    "        outputFile = csv.writer(open(outputFileName, \"w\", encoding='utf-8-sig', newline=''), delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "        outputFile.writerow(\n",
    "            ['username','date','retweets','favorites','text','geo','mentions','hashtags','id','permalink', 'emoji'])\n",
    "\n",
    "        print('Collecting tweets...\\n')\n",
    "\n",
    "        def receiveBuffer(tweets):\n",
    "            for t in tweets:\n",
    "                add_list = []\n",
    "                if (isinstance(t.emojis, list)):\n",
    "                    emoji = ' '.join(t.emojis)\n",
    "                else:\n",
    "                    emoji = t.emojis\n",
    "                for each in [t.username, t.date.strftime(\"%Y-%m-%d %H:%M\"), t.retweets, t.favorites, t.text, t.geo, t.mentions, t.hashtags, t.id, t.permalink, emoji]:\n",
    "                    add_list.append(each)\n",
    "                #mendapatkan tweet sesuai query dan memasukkan ke csv\n",
    "                outputFile.writerow(add_list)\n",
    "            print('%d tweets saved on file...\\n' % len(tweets))\n",
    "\n",
    "        got.manager.TweetManager.getTweets(tweetCriteria, receiveBuffer)\n",
    "\n",
    "    except :\n",
    "        print('Arguments parse error, failed collecting tweets')\n",
    "    finally:\n",
    "        print('Done. Output file generated \"%s\".' % outputFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweets...\n",
      "\n",
      "Twitter weird response. Try to see on browser: https://twitter.com/search?q=%20since%3A2019-06-01%20until%3A2019-07-02&src=typd\n",
      "Unexpected error: <class 'urllib.error.HTTPError'>\n",
      "Arguments parse error, failed collecting tweets\n",
      "Done. Output file generated \"TweetsTA.csv\".\n"
     ]
    }
   ],
   "source": [
    "query = [,\"--near\",\"Indonesia\",\"--within\",\"15mi\",\"--since\",\"2019-06-01\",\"--until\",\"2019-07-02\"]\n",
    "getTweet(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "tweets = pd.read_csv('TweetsTA.csv')\n",
    "# tweets.head(1)\n",
    "# tweets[['username','date','text','geo']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTRASI TWEETS DENGAN NER\n",
    "https://yudiwbs.wordpress.com/2018/03/29/ner-named-entity-recognition-dengan-anago-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ITS\\Tugas Akhir\\TA\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#mempersiapkan environment anago \n",
    "import anago\n",
    "from anago.reader import load_data_and_labels\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import gensim.models.keyedvectors as W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengakses file yang trainer, validator, dan tester\n",
    "namaFileTrain = \"train.txt\"\n",
    "namaFileValid = \"valid.txt\"\n",
    "namaFileTest = \"test.txt\"\n",
    "x_train, y_train = load_data_and_labels(namaFileTrain)\n",
    "x_valid, y_valid = load_data_and_labels(namaFileValid)\n",
    "x_test, y_test = load_data_and_labels(namaFileTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ITS\\Tugas Akhir\\TA\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\ITS\\Tugas Akhir\\TA\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\ITS\\Tugas Akhir\\TA\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "44/44 [==============================] - 11s 259ms/step - loss: 119.51120s - loss: 119.7\n",
      " - f1: 0.00\n",
      "Epoch 2/3\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 104.2277\n",
      " - f1: 0.00\n",
      "Epoch 3/3\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 101.7719\n",
      " - f1: 0.93\n"
     ]
    }
   ],
   "source": [
    "# karena hasil tdk konsisten, random seednya diisi manual\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "\n",
    "# mengatur parameter\n",
    "model = anago.Sequence(char_emb_size=25, word_emb_size=100, char_lstm_units=25,\n",
    "                       word_lstm_units=50, dropout=0.5, char_feature=True, crf=True,\n",
    "                       batch_size=20, optimizer='adam', learning_rate=0.001, lr_decay=0.9,\n",
    "                       clip_gradients=5.0, max_epoch=3, early_stopping=True, patience=3, train_embeddings=True,\n",
    "                       max_checkpoints_to_keep=5, log_dir=None)\n",
    "model.train(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluasi Test:\n",
      " - f1: 1.36\n",
      "{'words': ['Budi', 'Martami', 'kuliah', 'di', 'UPI', 'yang', 'berlokasi', 'di', 'Bandung'], 'entities': []}\n",
      "{'words': ['PDIP', 'yang', 'dikawal', 'Megawati', 'menang', 'dalam', 'Pilkada', 'DKI', 'Jakarta'], 'entities': []}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nEvaluasi Test:\")\n",
    "model.eval(x_test, y_test)\n",
    " \n",
    "words = 'Budi Martami kuliah di UPI yang berlokasi di Bandung'.split()\n",
    "print(model.analyze(words))\n",
    " \n",
    "words = 'PDIP yang dikawal Megawati menang dalam Pilkada DKI Jakarta'.split()\n",
    "print(model.analyze(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menyiapkan environment untuk preprocessing tweet\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import re\n",
    "import json\n",
    "import emoji\n",
    "# import spacy\n",
    "# from spacy.lang.id import Indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memuat tweet yang dikumpulkan sebelumnya (load dataset)\n",
    "tweets = pd.read_csv('TweetsTA.csv')\n",
    "# tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing Tweets\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# nlp = Indonesian()\n",
    "# pd.set_option('max_colwidth', 100)\n",
    "# stopwords = spacy.lang.id.stop_words.STOP_WORDS\n",
    "\n",
    "# load sastrawi's Bahasa Indonesia stopwords as variable called 'stopwords'\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()\n",
    "extender = [\"ada\", \"adalah\", \"adanya\", \"adapun\", \"agak\", \"agaknya\", \"agar\", \"akan\", \"akankah\", \"akhir\", \"akhiri\", \"akhirnya\", \"aku\", \"akulah\", \"amat\", \"amatlah\", \"anda\", \"andalah\", \"antar\", \"antara\", \"antaranya\", \"apa\", \"apaan\", \"apabila\", \"apakah\", \"apalagi\", \"apatah\", \"artinya\", \"asal\", \"asalkan\", \"atas\", \"atau\", \"ataukah\", \"ataupun\", \"awal\", \"awalnya\", \"bagai\", \"bagaikan\", \"bagaimana\", \"bagaimanakah\", \"bagaimanapun\", \"bagi\", \"bagian\", \"bahkan\", \"bahwa\", \"bahwasanya\", \"baik\", \"bakal\", \"bakalan\", \"balik\", \"banyak\", \"bapak\", \"baru\", \"bawah\", \"beberapa\", \"begini\", \"beginian\", \"beginikah\", \"beginilah\", \"begitu\", \"begitukah\", \"begitulah\", \"begitupun\", \"bekerja\", \"belakang\", \"belakangan\", \"belum\", \"belumlah\", \"benar\", \"benarkah\", \"benarlah\", \"berada\", \"berakhir\", \"berakhirlah\", \"berakhirnya\", \"berapa\", \"berapakah\", \"berapalah\", \"berapapun\", \"berarti\", \"berawal\", \"berbagai\", \"berdatangan\", \"beri\", \"berikan\", \"berikut\", \"berikutnya\", \"berjumlah\", \"berkali-kali\", \"berkata\", \"berkehendak\", \"berkeinginan\", \"berkenaan\", \"berlainan\", \"berlalu\", \"berlangsung\", \"berlebihan\", \"bermacam\", \"bermacam-macam\", \"bermaksud\", \"bermula\", \"bersama\", \"bersama-sama\", \"bersiap\", \"bersiap-siap\", \"bertanya\", \"bertanya-tanya\", \"berturut\", \"berturut-turut\", \"bertutur\", \"berujar\", \"berupa\", \"besar\", \"betul\", \"betulkah\", \"biasa\", \"biasanya\", \"bila\", \"bilakah\", \"bisa\", \"bisakah\", \"boleh\", \"bolehkah\", \"bolehlah\", \"buat\", \"bukan\", \"bukankah\", \"bukanlah\", \"bukannya\", \"bulan\", \"bung\", \"cara\", \"caranya\", \"cukup\", \"cukupkah\", \"cukuplah\", \"cuma\", \"dahulu\", \"dalam\", \"dan\", \"dapat\", \"dari\", \"daripada\", \"datang\", \"dekat\", \"demi\", \"demikian\", \"demikianlah\", \"dengan\", \"depan\", \"di\", \"dia\", \"diakhiri\", \"diakhirinya\", \"dialah\", \"diantara\", \"diantaranya\", \"diberi\", \"diberikan\", \"diberikannya\", \"dibuat\", \"dibuatnya\", \"didapat\", \"didatangkan\", \"digunakan\", \"diibaratkan\", \"diibaratkannya\", \"diingat\", \"diingatkan\", \"diinginkan\", \"dijawab\", \"dijelaskan\", \"dijelaskannya\", \"dikarenakan\", \"dikatakan\", \"dikatakannya\", \"dikerjakan\", \"diketahui\", \"diketahuinya\", \"dikira\", \"dilakukan\", \"dilalui\", \"dilihat\", \"dimaksud\", \"dimaksudkan\", \"dimaksudkannya\", \"dimaksudnya\", \"diminta\", \"dimintai\", \"dimisalkan\", \"dimulai\", \"dimulailah\", \"dimulainya\", \"dimungkinkan\", \"dini\", \"dipastikan\", \"diperbuat\", \"diperbuatnya\", \"dipergunakan\", \"diperkirakan\", \"diperlihatkan\", \"diperlukan\", \"diperlukannya\", \"dipersoalkan\", \"dipertanyakan\", \"dipunyai\", \"diri\", \"dirinya\", \"disampaikan\", \"disebut\", \"disebutkan\", \"disebutkannya\", \"disini\", \"disinilah\", \"ditambahkan\", \"ditandaskan\", \"ditanya\", \"ditanyai\", \"ditanyakan\", \"ditegaskan\", \"ditujukan\", \"ditunjuk\", \"ditunjuki\", \"ditunjukkan\", \"ditunjukkannya\", \"ditunjuknya\", \"dituturkan\", \"dituturkannya\", \"diucapkan\", \"diucapkannya\", \"diungkapkan\", \"dong\", \"dua\", \"dulu\", \"empat\", \"enggak\", \"enggaknya\", \"entah\", \"entahlah\", \"guna\", \"gunakan\", \"hal\", \"hampir\", \"hanya\", \"hanyalah\", \"hari\", \"harus\", \"haruslah\", \"harusnya\", \"hendak\", \"hendaklah\", \"hendaknya\", \"hingga\", \"ia\", \"ialah\", \"ibarat\", \"ibaratkan\", \"ibaratnya\", \"ibu\", \"ikut\", \"ingat\", \"ingat-ingat\", \"ingin\", \"inginkah\", \"inginkan\", \"ini\", \"inikah\", \"inilah\", \"itu\", \"itukah\", \"itulah\", \"jadi\", \"jadilah\", \"jadinya\", \"jangan\", \"jangankan\", \"janganlah\", \"jauh\", \"jawab\", \"jawaban\", \"jawabnya\", \"jelas\", \"jelaskan\", \"jelaslah\", \"jelasnya\", \"jika\", \"jikalau\", \"juga\", \"jumlah\", \"jumlahnya\", \"justru\", \"kala\", \"kalau\", \"kalaulah\", \"kalaupun\", \"kalian\", \"kami\", \"kamilah\", \"kamu\", \"kamulah\", \"kan\", \"kapan\", \"kapankah\", \"kapanpun\", \"karena\", \"karenanya\", \"kasus\", \"kata\", \"katakan\", \"katakanlah\", \"katanya\", \"ke\", \"keadaan\", \"kebetulan\", \"kecil\", \"kedua\", \"keduanya\", \"keinginan\", \"kelamaan\", \"kelihatan\", \"kelihatannya\", \"kelima\", \"keluar\", \"kembali\", \"kemudian\", \"kemungkinan\", \"kemungkinannya\", \"kenapa\", \"kepada\", \"kepadanya\", \"kesampaian\", \"keseluruhan\", \"keseluruhannya\", \"keterlaluan\", \"ketika\", \"khususnya\", \"kini\", \"kinilah\", \"kira\", \"kira-kira\", \"kiranya\", \"kita\", \"kitalah\", \"kok\", \"kurang\", \"lagi\", \"lagian\", \"lah\", \"lain\", \"lainnya\", \"lalu\", \"lama\", \"lamanya\", \"lanjut\", \"lanjutnya\", \"lebih\", \"lewat\", \"lima\", \"luar\", \"macam\", \"maka\", \"makanya\", \"makin\", \"malah\", \"malahan\", \"mampu\", \"mampukah\", \"mana\", \"manakala\", \"manalagi\", \"masa\", \"masalah\", \"masalahnya\", \"masih\", \"masihkah\", \"masing\", \"masing-masing\", \"mau\", \"maupun\", \"melainkan\", \"melakukan\", \"melalui\", \"melihat\", \"melihatnya\", \"memang\", \"memastikan\", \"memberi\", \"memberikan\", \"membuat\", \"memerlukan\", \"memihak\", \"meminta\", \"memintakan\", \"memisalkan\", \"memperbuat\", \"mempergunakan\", \"memperkirakan\", \"memperlihatkan\", \"mempersiapkan\", \"mempersoalkan\", \"mempertanyakan\", \"mempunyai\", \"memulai\", \"memungkinkan\", \"menaiki\", \"menambahkan\", \"menandaskan\", \"menanti\", \"menanti-nanti\", \"menantikan\", \"menanya\", \"menanyai\", \"menanyakan\", \"mendapat\", \"mendapatkan\", \"mendatang\", \"mendatangi\", \"mendatangkan\", \"menegaskan\", \"mengakhiri\", \"mengapa\", \"mengatakan\", \"mengatakannya\", \"mengenai\", \"mengerjakan\", \"mengetahui\", \"menggunakan\", \"menghendaki\", \"mengibaratkan\", \"mengibaratkannya\", \"mengingat\", \"mengingatkan\", \"menginginkan\", \"mengira\", \"mengucapkan\", \"mengucapkannya\", \"mengungkapkan\", \"menjadi\", \"menjawab\", \"menjelaskan\", \"menuju\", \"menunjuk\", \"menunjuki\", \"menunjukkan\", \"menunjuknya\", \"menurut\", \"menuturkan\", \"menyampaikan\", \"menyangkut\", \"menyatakan\", \"menyebutkan\", \"menyeluruh\", \"menyiapkan\", \"merasa\", \"mereka\", \"merekalah\", \"merupakan\", \"meski\", \"meskipun\", \"meyakini\", \"meyakinkan\", \"minta\", \"mirip\", \"misal\", \"misalkan\", \"misalnya\", \"mula\", \"mulai\", \"mulailah\", \"mulanya\", \"mungkin\", \"mungkinkah\", \"nah\", \"naik\", \"namun\", \"nanti\", \"nantinya\", \"nyaris\", \"nyatanya\", \"oleh\", \"olehnya\", \"pada\", \"padahal\", \"padanya\", \"pak\", \"paling\", \"panjang\", \"pantas\", \"para\", \"pasti\", \"pastilah\", \"penting\", \"pentingnya\", \"per\", \"percuma\", \"perlu\", \"perlukah\", \"perlunya\", \"pernah\", \"persoalan\", \"pertama\", \"pertama-tama\", \"pertanyaan\", \"pertanyakan\", \"pihak\", \"pihaknya\", \"pukul\", \"pula\", \"pun\", \"punya\", \"rasa\", \"rasanya\", \"rata\", \"rupanya\", \"saat\", \"saatnya\", \"saja\", \"sajalah\", \"saling\", \"sama\", \"sama-sama\", \"sambil\", \"sampai\", \"sampai-sampai\", \"sampaikan\", \"sana\", \"sangat\", \"sangatlah\", \"satu\", \"saya\", \"sayalah\", \"se\", \"sebab\", \"sebabnya\", \"sebagai\", \"sebagaimana\", \"sebagainya\", \"sebagian\", \"sebaik\", \"sebaik-baiknya\", \"sebaiknya\", \"sebaliknya\", \"sebanyak\", \"sebegini\", \"sebegitu\", \"sebelum\", \"sebelumnya\", \"sebenarnya\", \"seberapa\", \"sebesar\", \"sebetulnya\", \"sebisanya\", \"sebuah\", \"sebut\", \"sebutlah\", \"sebutnya\", \"secara\", \"secukupnya\", \"sedang\", \"sedangkan\", \"sedemikian\", \"sedikit\", \"sedikitnya\", \"seenaknya\", \"segala\", \"segalanya\", \"segera\", \"seharusnya\", \"sehingga\", \"seingat\", \"sejak\", \"sejauh\", \"sejenak\", \"sejumlah\", \"sekadar\", \"sekadarnya\", \"sekali\", \"sekali-kali\", \"sekalian\", \"sekaligus\", \"sekalipun\", \"sekarang\", \"sekarang\", \"sekecil\", \"seketika\", \"sekiranya\", \"sekitar\", \"sekitarnya\", \"sekurang-kurangnya\", \"sekurangnya\", \"sela\", \"selain\", \"selaku\", \"selalu\", \"selama\", \"selama-lamanya\", \"selamanya\", \"selanjutnya\", \"seluruh\", \"seluruhnya\", \"semacam\", \"semakin\", \"semampu\", \"semampunya\", \"semasa\", \"semasih\", \"semata\", \"semata-mata\", \"semaunya\", \"sementara\", \"semisal\", \"semisalnya\", \"sempat\", \"semua\", \"semuanya\", \"semula\", \"sendiri\", \"sendirian\", \"sendirinya\", \"seolah\", \"seolah-olah\", \"seorang\", \"sepanjang\", \"sepantasnya\", \"sepantasnyalah\", \"seperlunya\", \"seperti\", \"sepertinya\", \"sepihak\", \"sering\", \"seringnya\", \"serta\", \"serupa\", \"sesaat\", \"sesama\", \"sesampai\", \"sesegera\", \"sesekali\", \"seseorang\", \"sesuatu\", \"sesuatunya\", \"sesudah\", \"sesudahnya\", \"setelah\", \"setempat\", \"setengah\", \"seterusnya\", \"setiap\", \"setiba\", \"setibanya\", \"setidak-tidaknya\", \"setidaknya\", \"setinggi\", \"seusai\", \"sewaktu\", \"siap\", \"siapa\", \"siapakah\", \"siapapun\", \"sini\", \"sinilah\", \"soal\", \"soalnya\", \"suatu\", \"sudah\", \"sudahkah\", \"sudahlah\", \"supaya\", \"tadi\", \"tadinya\", \"tahu\", \"tahun\", \"tak\", \"tambah\", \"tambahnya\", \"tampak\", \"tampaknya\", \"tandas\", \"tandasnya\", \"tanpa\", \"tanya\", \"tanyakan\", \"tanyanya\", \"tapi\", \"tegas\", \"tegasnya\", \"telah\", \"tempat\", \"tengah\", \"tentang\", \"tentu\", \"tentulah\", \"tentunya\", \"tepat\", \"terakhir\", \"terasa\", \"terbanyak\", \"terdahulu\", \"terdapat\", \"terdiri\", \"terhadap\", \"terhadapnya\", \"teringat\", \"teringat-ingat\", \"terjadi\", \"terjadilah\", \"terjadinya\", \"terkira\", \"terlalu\", \"terlebih\", \"terlihat\", \"termasuk\", \"ternyata\", \"tersampaikan\", \"tersebut\", \"tersebutlah\", \"tertentu\", \"tertuju\", \"terus\", \"terutama\", \"tetap\", \"tetapi\", \"tiap\", \"tiba\", \"tiba-tiba\", \"tidak\", \"tidakkah\", \"tidaklah\", \"tiga\", \"tinggi\", \"toh\", \"tunjuk\", \"turut\", \"tutur\", \"tuturnya\", \"ucap\", \"ucapnya\", \"ujar\", \"ujarnya\", \"umum\", \"umumnya\", \"ungkap\", \"ungkapnya\", \"untuk\", \"usah\", \"usai\", \"waduh\", \"wah\", \"wahai\", \"waktu\", \"waktunya\", \"walau\", \"walaupun\", \"wong\", \"yaitu\", \"yakin\", \"yakni\", \"yang\"]\n",
    "stopwords.extend(extender)\n",
    "# print(stopwords)\n",
    "def normalize_repeated_char(tokenized_sentence):\n",
    "    alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "                'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "                'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    tokenized_sentence = ' '.join(tokenized_sentence)\n",
    "    for i in range(len(alphabet)):\n",
    "        charac_long = 5\n",
    "        while charac_long >= 2:\n",
    "            char = alphabet[i] * charac_long\n",
    "            tokenized_sentence = tokenized_sentence.replace(char, alphabet[i])\n",
    "            charac_long -= 1        \n",
    "    tokenized_sentence = tokenized_sentence.split()    \n",
    "    return tokenized_sentence\n",
    "\n",
    "def delete_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    for word in sentence:\n",
    "        for letter in word:\n",
    "            print(letter)\n",
    "#             letter = emoji_pattern.sub(r'', letter)\n",
    "#     deleted_emoji = emoji_pattern.sub(r'', ContentNoSlang)\n",
    "#     return deleted_emoji\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def preprocessing_tweets_first(text):\n",
    "    clean = html.unescape(text) #Handling Unicode HTML\n",
    "    clean = re.sub(\"[0-9]\", \"\", clean) #RemoveNumber\n",
    "    clean = re.sub(r\"http\\S+\", \"\", clean) #Remove links\n",
    "    clean = deEmojify(clean) #Remove Emoji\n",
    "    clean = \" \".join(re.findall(\"[#a-zA-Z]{3,}\", clean)) #Remove Puntc\n",
    "    content = list()\n",
    "    content.append(clean)\n",
    "    return \" \".join(content)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean'] = tweets['text'].apply(preprocessing_tweets_first)\n",
    "tweets = tweets.drop([\"retweets\", \"favorites\", \"mentions\", \"hashtags\", \"id\", \"permalink\", \"emoji\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PenguntaiKata</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Jadi pindah negara gak ? Jokowi menang tuh üòÇüòÇüòÇüòÇ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jadi pindah negara gak Jokowi menang tuh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SutrisnoAhmadd</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Bilg curg tpi ga bisa buktikan itu fitnah..tau...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bilg curg tpi bisa buktikan itu fitnah tau kan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RencanaTrading</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Thanks Jul.  Baru sadar kalau ada berita itu. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks Jul Baru sadar kalau ada berita itu Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anwar_suprijadi</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Mestinya dulu segera diruwat..  Trus ganti nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mestinya dulu segera diruwat Trus ganti nama j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TlatahBocah</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>5 - 7/7/19 Festival Lima Gunung XVIII | Padepo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Festival Lima Gunung XVIII Padepokan Seni Tjip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doenklanus88</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Ya lebih baik hadir,itu memberikan contoh jiwa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lebih baik hadir itu memberikan contoh jiwa ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GATRA_News</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Teken Perpres 37/2019, Jokowi Dinilai Buta Per...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teken Perpres Jokowi Dinilai Buta Pertahanan P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>srisumartie</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Jangan remehkan \"Nafsunya para ABJ\" üòÅüòÅüòÅ mpe se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jangan remehkan Nafsunya para ABJ mpe segituny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>veyedov</td>\n",
       "      <td>2019-07-02 06:58</td>\n",
       "      <td>Lu dirampok dijarah, anak2 lu dipukulin trus l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dirampok dijarah anak dipukulin trus bilang se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AfanHakimi</td>\n",
       "      <td>2019-07-02 06:58</td>\n",
       "      <td>Mantap Pak Jokowi..Abaikan saja para kampret, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mantap Pak Jokowi Abaikan saja para kampret sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username              date  \\\n",
       "0    PenguntaiKata  2019-07-02 06:59   \n",
       "1   SutrisnoAhmadd  2019-07-02 06:59   \n",
       "2   RencanaTrading  2019-07-02 06:59   \n",
       "3  anwar_suprijadi  2019-07-02 06:59   \n",
       "4      TlatahBocah  2019-07-02 06:59   \n",
       "5     doenklanus88  2019-07-02 06:59   \n",
       "6       GATRA_News  2019-07-02 06:59   \n",
       "7      srisumartie  2019-07-02 06:59   \n",
       "8          veyedov  2019-07-02 06:58   \n",
       "9       AfanHakimi  2019-07-02 06:58   \n",
       "\n",
       "                                                text  geo  \\\n",
       "0    Jadi pindah negara gak ? Jokowi menang tuh üòÇüòÇüòÇüòÇ  NaN   \n",
       "1  Bilg curg tpi ga bisa buktikan itu fitnah..tau...  NaN   \n",
       "2  Thanks Jul.  Baru sadar kalau ada berita itu. ...  NaN   \n",
       "3  Mestinya dulu segera diruwat..  Trus ganti nam...  NaN   \n",
       "4  5 - 7/7/19 Festival Lima Gunung XVIII | Padepo...  NaN   \n",
       "5  Ya lebih baik hadir,itu memberikan contoh jiwa...  NaN   \n",
       "6  Teken Perpres 37/2019, Jokowi Dinilai Buta Per...  NaN   \n",
       "7  Jangan remehkan \"Nafsunya para ABJ\" üòÅüòÅüòÅ mpe se...  NaN   \n",
       "8  Lu dirampok dijarah, anak2 lu dipukulin trus l...  NaN   \n",
       "9  Mantap Pak Jokowi..Abaikan saja para kampret, ...  NaN   \n",
       "\n",
       "                                               clean  \n",
       "0           Jadi pindah negara gak Jokowi menang tuh  \n",
       "1  Bilg curg tpi bisa buktikan itu fitnah tau kan...  \n",
       "2  Thanks Jul Baru sadar kalau ada berita itu Ter...  \n",
       "3  Mestinya dulu segera diruwat Trus ganti nama j...  \n",
       "4  Festival Lima Gunung XVIII Padepokan Seni Tjip...  \n",
       "5  lebih baik hadir itu memberikan contoh jiwa ke...  \n",
       "6  Teken Perpres Jokowi Dinilai Buta Pertahanan P...  \n",
       "7  Jangan remehkan Nafsunya para ABJ mpe segituny...  \n",
       "8  dirampok dijarah anak dipukulin trus bilang se...  \n",
       "9  Mantap Pak Jokowi Abaikan saja para kampret sa...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EKSTRAKSI TRENDING ISSUE DARI TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrasi Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PenguntaiKata</td>\n",
       "      <td>2019-07-02 06:59</td>\n",
       "      <td>Jadi pindah negara gak ? Jokowi menang tuh üòÇüòÇüòÇüòÇ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jadi pindah negara gak Jokowi menang tuh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username              date  \\\n",
       "0  PenguntaiKata  2019-07-02 06:59   \n",
       "\n",
       "                                              text  geo  \\\n",
       "0  Jadi pindah negara gak ? Jokowi menang tuh üòÇüòÇüòÇüòÇ  NaN   \n",
       "\n",
       "                                      clean  \n",
       "0  Jadi pindah negara gak Jokowi menang tuh  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet = tweets['clean'].tolist()\n",
    "# i=0\n",
    "# for a in tweet:\n",
    "#     if len(a.split()) > 2:\n",
    "#         model.analyze(a.split())\n",
    "#         print(i)\n",
    "#         i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtrasi tweets dengan menggunakan NER\n",
    "index = 0\n",
    "tweets_with_entity = pd.DataFrame(columns=['text', 'index'])\n",
    "for sent in tweets['clean'].tolist():\n",
    "    if len(sent.split()) >= 3:\n",
    "        result = model.analyze(sent.split())\n",
    "        if result.get(\"entities\") != []:\n",
    "            tweets_with_entity = tweets_with_entity.append({'text':sent, 'index': index}, ignore_index=True)\n",
    "#             print(sent)\n",
    "#     print(index)\n",
    "#     index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>budaya barat tuh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text index\n",
       "0  budaya barat tuh     0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_entity.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CSV File\n",
    "with open('clean.csv', 'w', encoding=\"utf-8\") as csv_tweet:\n",
    "    fieldnames_tweet = ['id','text']\n",
    "    writer_tweet = csv.DictWriter(csv_tweet, fieldnames=fieldnames_tweet)\n",
    "    writer_tweet.writeheader()\n",
    "    id = 0\n",
    "    for index, tweet in tweets.iterrows():\n",
    "        words = tweet[\"clean\"].split()\n",
    "        if (len(words)) > 2:\n",
    "            writer_tweet.writerow({\n",
    "                'id': index, \n",
    "                'text': tweet[\"clean\"]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing tweets tahap2\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def stopword(words):\n",
    "    clean = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            clean.append(word)\n",
    "    return (\" \".join(clean))\n",
    "\n",
    "\n",
    "def normalize_slang_words(text):\n",
    "    slang_word_dict = json.loads(open('slang_word_dict.txt', 'r').read())\n",
    "    words = text.split()\n",
    "    for index in range(len(words)):\n",
    "        for key, value in slang_word_dict.items():\n",
    "            for v in value:\n",
    "                if words[index] == v:\n",
    "                    words[index] = key\n",
    "                else:\n",
    "                    continue \n",
    "    content = list()\n",
    "    content.append(words)\n",
    "\n",
    "    return \" \".join(content)\n",
    "\n",
    "def preprocessing_tweets_second(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    clean   = stemmer.stem(text) \n",
    "    clean = re.sub(\"#\", \"\", clean)\n",
    "    clean = stopword(clean.split()) #Stopword\n",
    "    clean\n",
    "#     clean = normalize_slang_words(clean)\n",
    "    return clean\n",
    "\n",
    "tweets_with_entity['clean'] = tweets_with_entity['text'].apply(preprocessing_tweets_second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_with_entity.head(10)\n",
    "export_csv = tweets_with_entity.to_csv (r'.\\tweets_with_entity.csv', encoding=\"mbcs\", header=True) #Don't forget to add '.csv' at the end of the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "#     print(filtered_tokens)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 91.8 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1, max_features=200000,\n",
    "                                 min_df = 0.2, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(tweets_with_entity['clean'].tolist()) #fit the vectorizer to synopses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>barat</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>barat tuh</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>budaya</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>budaya barat</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>budaya barat tuh</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tuh</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tfidf\n",
       "barat             0.408248\n",
       "barat tuh         0.408248\n",
       "budaya            0.408248\n",
       "budaya barat      0.408248\n",
       "budaya barat tuh  0.408248\n",
       "tuh               0.408248"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=tfidf_matrix[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'KMedoids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9de9f2640b57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mc_sim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mKMedoids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'KMedoids'"
     ]
    }
   ],
   "source": [
    "def cosine_distance(a, b, weigth = []):\n",
    "    if len(weigth) == 0:\n",
    "        weigth = np.array([1] * len(a))\n",
    "    c_sim = np.sum(a * b * weigth) / (np.sqrt(sum(a ** 2)) * np.sqrt(sum(b ** 2)))\n",
    "    return 1 - c_sim\n",
    "\n",
    "import KMedoids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-fcfef3eb3a8b>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-fcfef3eb3a8b>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    for i, feat in enumerate(features): k.minute, 0)\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def silhouette(features, cluster_id):\n",
    "    total = 0\n",
    "    n = len(np.unique(cluster_id))\n",
    "    for i, feat in enumerate(features): k.minute, 0)\n",
    "        inner = 0\n",
    "        outer = np.inf\n",
    "        for j in range(n):\n",
    "            dist = euclidean_distances(feat,features [cluster_id == j])[0]\n",
    "        if j == cluster_id[i]:\n",
    "            inner += dist.mean()\n",
    "        else\n",
    "            outer = min(outer, dist.mean())\n",
    "        total += ((outer - inner) / max(outer,inner) if max(outer, inner) != 0 else 0)\n",
    "    return total / len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000, min_df=0.2 )\n",
    "# X = vectorizer.fit_transform(tweets_with_entity['clean'].tolist())\n",
    "# # print(vectorizer.get_feature_names())\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_feature_names not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-7a39ab56ca1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# print(tokenize_only(tweets['text'].tolist()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ITS\\Tugas Akhir\\TA\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: get_feature_names not found"
     ]
    }
   ],
   "source": [
    "# print(tokenize_only(tweets['text'].tolist()))\n",
    "print(tfidf_matrix.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e27d3f015b70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrue_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrue_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# documents = indonesia['clean'].tolist()\n",
    "# vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "# X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " aamiin hbdjokowi\n",
      " aamiin robbal\n",
      " aamiin\n",
      " aamiin moga lancar\n",
      " aamiin yra\n",
      " aamiin robbal alamin\n",
      " aamiin moga\n",
      " aaaaaamiiinnnn\n",
      "Cluster 1:\n",
      " aamiin\n",
      " aamiin yra\n",
      " aamiin robbal alamin\n",
      " aamiin robbal\n",
      " aamiin moga lancar\n",
      " aamiin moga\n",
      " aamiin hbdjokowi\n",
      " aaaaaamiiinnnn\n",
      "Cluster 2:\n",
      " aamiin robbal\n",
      " aamiin robbal alamin\n",
      " aamiin yra\n",
      " aaaaaamiiinnnn\n",
      " aamiin\n",
      " aamiin moga lancar\n",
      " aamiin hbdjokowi\n",
      " aamiin moga\n",
      "Cluster 3:\n",
      " aaaaaamiiinnnn\n",
      " aamiin\n",
      " aamiin hbdjokowi\n",
      " aamiin moga lancar\n",
      " aamiin robbal\n",
      " aamiin moga\n",
      " aamiin yra\n",
      " aamiin robbal alamin\n",
      "Cluster 4:\n",
      " aamiin moga\n",
      " aamiin\n",
      " aaaaaamiiinnnn\n",
      " aamiin moga lancar\n",
      " aamiin hbdjokowi\n",
      " aamiin robbal\n",
      " aamiin yra\n",
      " aamiin robbal alamin\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"you are beautiful.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"jokowi terbaik.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmedoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "representative words / sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trending topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMEDOIDS\n",
    "https://github.com/letiantian/kmedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing and tokenizing\n",
    "l_A = corpus[0].lower().split()\n",
    "l_B = corpus[1].lower().split()\n",
    "l_C = corpus[2].lower().split()\n",
    "\n",
    "# Calculating bag of words\n",
    "word_set = set(l_A).union(set(l_B)).union(set(l_C))\n",
    "\n",
    "word_dict_A = dict.fromkeys(word_set, 0)\n",
    "word_dict_B = dict.fromkeys(word_set, 0)\n",
    "word_dict_C = dict.fromkeys(word_set, 0)\n",
    "\n",
    "for word in l_A:\n",
    "    word_dict_A[word] += 1\n",
    "\n",
    "for word in l_B:\n",
    "    word_dict_B[word] += 1\n",
    "\n",
    "for word in l_C:\n",
    "word_dict_C[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(word_dict, l):\n",
    "    tf = {}\n",
    "    sum_nk = len(l)\n",
    "    for word, count in word_dict.items():\n",
    "        tf[word] = count/sum_nk\n",
    "    return tf\n",
    "  \n",
    "tf_A = compute_tf(word_dict_A, l_A)\n",
    "tf_B = compute_tf(word_dict_B, l_B)\n",
    "tf_C = compute_tf(word_dict_C, l_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(strings_list):\n",
    "    n = len(strings_list)\n",
    "    idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "    for l in strings_list:\n",
    "        for word, count in l.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word, v in idf.items():\n",
    "        idf[word] = log(n / float(v))\n",
    "    return idf\n",
    "    \n",
    "idf = compute_idf([word_dict_A, word_dict_B, word_dict_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "    for word, v in tf.items():\n",
    "        tf_idf[word] = v * idf[word]\n",
    "    return tf_idf\n",
    "    \n",
    "tf_idf_A = compute_tf_idf(tf_A, idf)\n",
    "tf_idf_B = compute_tf_idf(tf_B, idf)\n",
    "tf_idf_C = compute_tf_idf(tf_C, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessing)\n",
    "tfidf = tfidf_vectorizer.fit_transform(all_text)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2).fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMEDOIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "import kmedoids\n",
    "\n",
    "# 3 points in dataset\n",
    "data = np.array([[1,1], \n",
    "                [2,2], \n",
    "                [10,10]])\n",
    "\n",
    "# distance matrix\n",
    "D = pairwise_distances(data, metric='euclidean')\n",
    "\n",
    "# split into 2 clusters\n",
    "M, C = kmedoids.kMedoids(D, 2)\n",
    "\n",
    "print('medoids:')\n",
    "for point_idx in M:\n",
    "    print( data[point_idx] )\n",
    "\n",
    "print('')\n",
    "print('clustering result:')\n",
    "for label in C:\n",
    "    for point_idx in C[label]:\n",
    "        print('label {0}:„ÄÄ{1}'.format(label, data[point_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTRASI BERITA BERDASARKAN TRENDING ISSUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gokhanatil.com/2017/11/python-for-data-science-importing-xml-to-pandas-dataframe.html\n",
    "import xml.etree.ElementTree as et \n",
    "import os\n",
    "directory = \"./ScrapeNews/berita/\"\n",
    "df = pd.DataFrame(columns=['judul','id','tanggal','kata_kunci','√≠si'])\n",
    "\n",
    "for berita in os.listdir(directory):\n",
    "    filename = directory+berita\n",
    "    xtree = et.parse(filename).getroot()\n",
    "    judul = xtree.find('judul').text\n",
    "    ids = xtree.find('id').text\n",
    "    tanggal = xtree.find('tanggal').text\n",
    "    kata_kunci = xtree.find('kata_kunci').text\n",
    "    isi = xtree.find('isi').text\n",
    "    df = df.append({'judul':judul,'id':ids,'tanggal':tanggal,'kata_kunci':kata_kunci,'√≠si':isi},ignore_index=True)\n",
    "#     df2 = pd.DataFrame([judul,id,tanggal,kata_kunci,isi], columns=['judul','id','tanggal','kata_kunci','√≠si'])\n",
    "#     df = df.append(df2)\n",
    "\n",
    "pd.set_option('max_colwidth', 30)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>id</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>kata_kunci</th>\n",
       "      <th>√≠si</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kronologi Kasus Korupsi Pl...</td>\n",
       "      <td>POL_POL_1</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>michel,platini,uefa,piala,...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Persib Imbang 1-1 dengan T...</td>\n",
       "      <td>POL_POL_2</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>persib,bandung,ps,tira,per...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jadwal Copa America 2019: ...</td>\n",
       "      <td>POL_POL_3</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>copa,america,copa,america,...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kasus Korupsi Platini Sere...</td>\n",
       "      <td>POL_POL_4</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>michel,platini,uefa,fifa,n...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICW: Setya Novanto Jadi Ke...</td>\n",
       "      <td>POL_PP_02_025</td>\n",
       "      <td>17/05/16</td>\n",
       "      <td></td>\n",
       "      <td>\\n\\t Terpilihnya Setya Nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Penutupan Kongres PDIP Bah...</td>\n",
       "      <td>POL_PP_02_1089</td>\n",
       "      <td>11/4/15</td>\n",
       "      <td>RapimnasPDIP,Pilkada2015,...</td>\n",
       "      <td>\\n Penutupan Kongres IV Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Disambut Meriah di Rakerna...</td>\n",
       "      <td>POL_PP_02_1090</td>\n",
       "      <td>10/01/16</td>\n",
       "      <td>RapimnasPDIP,Ahok</td>\n",
       "      <td>\\n Gubernur DKI Jakarta, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Silaturahmi Nasional Parta...</td>\n",
       "      <td>POL_PP_02_1091</td>\n",
       "      <td>02/11/15</td>\n",
       "      <td>SilatnasGolkar,PecahGolka...</td>\n",
       "      <td>\\n Wakil Sekretaris Jender...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SBY Apresiasi Jokowi yang ...</td>\n",
       "      <td>POL_PP_02_1092</td>\n",
       "      <td>12/05/15</td>\n",
       "      <td>KongresDemokrat,Jokowi,SB...</td>\n",
       "      <td>\\n Kongres IV Partai Demok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Prabowo Batal Hadir, Petin...</td>\n",
       "      <td>POL_PP_02_1093</td>\n",
       "      <td>12/05/15</td>\n",
       "      <td>KongresDemokrat,KoalisiMe...</td>\n",
       "      <td>\\nKetua Umum Gerindra Prab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           judul              id     tanggal  \\\n",
       "0  Kronologi Kasus Korupsi Pl...       POL_POL_1  18/06/2019   \n",
       "1  Persib Imbang 1-1 dengan T...       POL_POL_2  18/06/2019   \n",
       "2  Jadwal Copa America 2019: ...       POL_POL_3  18/06/2019   \n",
       "3  Kasus Korupsi Platini Sere...       POL_POL_4  18/06/2019   \n",
       "4  ICW: Setya Novanto Jadi Ke...   POL_PP_02_025    17/05/16   \n",
       "5  Penutupan Kongres PDIP Bah...  POL_PP_02_1089     11/4/15   \n",
       "6  Disambut Meriah di Rakerna...  POL_PP_02_1090    10/01/16   \n",
       "7  Silaturahmi Nasional Parta...  POL_PP_02_1091    02/11/15   \n",
       "8  SBY Apresiasi Jokowi yang ...  POL_PP_02_1092    12/05/15   \n",
       "9  Prabowo Batal Hadir, Petin...  POL_PP_02_1093    12/05/15   \n",
       "\n",
       "                      kata_kunci                            √≠si  \n",
       "0  michel,platini,uefa,piala,...  Jakarta, CNN Indonesia -- ...  \n",
       "1  persib,bandung,ps,tira,per...  Jakarta, CNN Indonesia -- ...  \n",
       "2  copa,america,copa,america,...  Jakarta, CNN Indonesia -- ...  \n",
       "3  michel,platini,uefa,fifa,n...  Jakarta, CNN Indonesia -- ...  \n",
       "4                                 \\n\\t Terpilihnya Setya Nov...  \n",
       "5   RapimnasPDIP,Pilkada2015,...  \\n Penutupan Kongres IV Pa...  \n",
       "6             RapimnasPDIP,Ahok   \\n Gubernur DKI Jakarta, B...  \n",
       "7   SilatnasGolkar,PecahGolka...  \\n Wakil Sekretaris Jender...  \n",
       "8   KongresDemokrat,Jokowi,SB...  \\n Kongres IV Partai Demok...  \n",
       "9   KongresDemokrat,KoalisiMe...  \\nKetua Umum Gerindra Prab...  "
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>id</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>kata_kunci</th>\n",
       "      <th>√≠si</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kronologi Kasus Korupsi Pl...</td>\n",
       "      <td>POL_POL_1</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>michel,platini,uefa,piala,...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "      <td>jakarta cnn indonesia pena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Persib Imbang 1-1 dengan T...</td>\n",
       "      <td>POL_POL_2</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>persib,bandung,ps,tira,per...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "      <td>jakarta cnn indonesia pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jadwal Copa America 2019: ...</td>\n",
       "      <td>POL_POL_3</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>copa,america,copa,america,...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "      <td>jakarta cnn indonesia timn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kasus Korupsi Platini Sere...</td>\n",
       "      <td>POL_POL_4</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>michel,platini,uefa,fifa,n...</td>\n",
       "      <td>Jakarta, CNN Indonesia -- ...</td>\n",
       "      <td>jakarta cnn indonesia nama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICW: Setya Novanto Jadi Ke...</td>\n",
       "      <td>POL_PP_02_025</td>\n",
       "      <td>17/05/16</td>\n",
       "      <td></td>\n",
       "      <td>\\n\\t Terpilihnya Setya Nov...</td>\n",
       "      <td>terpilihnya setya novanto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Penutupan Kongres PDIP Bah...</td>\n",
       "      <td>POL_PP_02_1089</td>\n",
       "      <td>11/4/15</td>\n",
       "      <td>RapimnasPDIP,Pilkada2015,...</td>\n",
       "      <td>\\n Penutupan Kongres IV Pa...</td>\n",
       "      <td>penutupan kongres partai d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Disambut Meriah di Rakerna...</td>\n",
       "      <td>POL_PP_02_1090</td>\n",
       "      <td>10/01/16</td>\n",
       "      <td>RapimnasPDIP,Ahok</td>\n",
       "      <td>\\n Gubernur DKI Jakarta, B...</td>\n",
       "      <td>gubernur dki jakarta basuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Silaturahmi Nasional Parta...</td>\n",
       "      <td>POL_PP_02_1091</td>\n",
       "      <td>02/11/15</td>\n",
       "      <td>SilatnasGolkar,PecahGolka...</td>\n",
       "      <td>\\n Wakil Sekretaris Jender...</td>\n",
       "      <td>wakil sekretaris jenderal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SBY Apresiasi Jokowi yang ...</td>\n",
       "      <td>POL_PP_02_1092</td>\n",
       "      <td>12/05/15</td>\n",
       "      <td>KongresDemokrat,Jokowi,SB...</td>\n",
       "      <td>\\n Kongres IV Partai Demok...</td>\n",
       "      <td>kongres partai demokrat ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Prabowo Batal Hadir, Petin...</td>\n",
       "      <td>POL_PP_02_1093</td>\n",
       "      <td>12/05/15</td>\n",
       "      <td>KongresDemokrat,KoalisiMe...</td>\n",
       "      <td>\\nKetua Umum Gerindra Prab...</td>\n",
       "      <td>ketua umum gerindra prabow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           judul              id     tanggal  \\\n",
       "0  Kronologi Kasus Korupsi Pl...       POL_POL_1  18/06/2019   \n",
       "1  Persib Imbang 1-1 dengan T...       POL_POL_2  18/06/2019   \n",
       "2  Jadwal Copa America 2019: ...       POL_POL_3  18/06/2019   \n",
       "3  Kasus Korupsi Platini Sere...       POL_POL_4  18/06/2019   \n",
       "4  ICW: Setya Novanto Jadi Ke...   POL_PP_02_025    17/05/16   \n",
       "5  Penutupan Kongres PDIP Bah...  POL_PP_02_1089     11/4/15   \n",
       "6  Disambut Meriah di Rakerna...  POL_PP_02_1090    10/01/16   \n",
       "7  Silaturahmi Nasional Parta...  POL_PP_02_1091    02/11/15   \n",
       "8  SBY Apresiasi Jokowi yang ...  POL_PP_02_1092    12/05/15   \n",
       "9  Prabowo Batal Hadir, Petin...  POL_PP_02_1093    12/05/15   \n",
       "\n",
       "                      kata_kunci                            √≠si  \\\n",
       "0  michel,platini,uefa,piala,...  Jakarta, CNN Indonesia -- ...   \n",
       "1  persib,bandung,ps,tira,per...  Jakarta, CNN Indonesia -- ...   \n",
       "2  copa,america,copa,america,...  Jakarta, CNN Indonesia -- ...   \n",
       "3  michel,platini,uefa,fifa,n...  Jakarta, CNN Indonesia -- ...   \n",
       "4                                 \\n\\t Terpilihnya Setya Nov...   \n",
       "5   RapimnasPDIP,Pilkada2015,...  \\n Penutupan Kongres IV Pa...   \n",
       "6             RapimnasPDIP,Ahok   \\n Gubernur DKI Jakarta, B...   \n",
       "7   SilatnasGolkar,PecahGolka...  \\n Wakil Sekretaris Jender...   \n",
       "8   KongresDemokrat,Jokowi,SB...  \\n Kongres IV Partai Demok...   \n",
       "9   KongresDemokrat,KoalisiMe...  \\nKetua Umum Gerindra Prab...   \n",
       "\n",
       "                           clean  \n",
       "0  jakarta cnn indonesia pena...  \n",
       "1  jakarta cnn indonesia pers...  \n",
       "2  jakarta cnn indonesia timn...  \n",
       "3  jakarta cnn indonesia nama...  \n",
       "4  terpilihnya setya novanto ...  \n",
       "5  penutupan kongres partai d...  \n",
       "6  gubernur dki jakarta basuk...  \n",
       "7  wakil sekretaris jenderal ...  \n",
       "8  kongres partai demokrat ho...  \n",
       "9  ketua umum gerindra prabow...  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing berita\n",
    "# factory = StopWordRemoverFactory()\n",
    "# stopwords = factory.get_stop_words()\n",
    "# stopwords.extend(extender)\n",
    "\n",
    "\n",
    "def preprocessing_news(text):\n",
    "    clean = html.unescape(text) #Handling Unicode HTML\n",
    "    clean = re.sub(\"[0-9]\", \"\", clean) #RemoveNumber\n",
    "    clean = \" \".join(re.findall(\"[#a-zA-Z]{3,}\", clean)) #Remove Puntc\n",
    "    clean = clean.lower()\n",
    "    content = list()\n",
    "    content.append(clean)\n",
    "    return \" \".join(content)\n",
    "\n",
    "# def preprocessing_news(text):\n",
    "#     clean = html.unescape(text) #Handling Unicode HTML\n",
    "#     clean = \" \".join(re.findall(\"[#a-zA-Z]{3,}\", clean)) #Remove Puntc\n",
    "#     clean = text.lower() #lowercase # permrosesan NER sangat dipengaruhi huruf kapital\n",
    "#     clean = stopword(clean) #Stopword\n",
    "    return clean\n",
    "#     content = list()\n",
    "#     content.append(clean)\n",
    "#     return \" \".join(content)\n",
    "\n",
    "df['clean'] = df['√≠si'].apply(preprocessing_news)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = df.to_csv (r'.\\berita.csv', encoding=\"mbcs\", header=True) #Don't forget to add '.csv' at the end of the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def news_filtering(news_collection, terms):\n",
    "    trending_news = list()\n",
    "    for news in news_collection:\n",
    "        result = False\n",
    "        for sent in news:\n",
    "            result = any (term in terms for word in sent)\n",
    "            if result:\n",
    "                trending_news.append(news)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEMBOBOTAN KALIMAT BERITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "#     print(filtered_tokens)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 menghitung Term Frequency\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df = 0.2, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(tweets_with_entity['clean'].tolist()) #fit the vectorizer to synopses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2 TF-IDF\n",
    "def frequency_scores(self, article_text):\n",
    "    \"\"\" Individual (stemmed) word weights are then calculated for each\n",
    "        word in the given article. Sentences are scored as the sum of their TF-IDF word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add our document into the model so we can retrieve scores\n",
    "    response = self._tfidf.transform([article_text])\n",
    "    feature_names = self._tfidf.get_feature_names() # these are just stemmed words\n",
    "\n",
    "    word_prob = {}  # TF-IDF individual word probabilities\n",
    "    for col in response.nonzero()[1]:\n",
    "        word_prob[feature_names[col]] = response[0, col]\n",
    "    if DEBUG:\n",
    "        print(word_prob)\n",
    "\n",
    "    sent_scores = []\n",
    "    for sentence in self.split_into_sentences(article_text):\n",
    "        score = 0\n",
    "        sent_tokens = self.tokenize_and_stem(sentence)\n",
    "        for token in (t for t in sent_tokens if t in word_prob):\n",
    "            score += word_prob[token]\n",
    "\n",
    "        # Normalize score by length of sentence, since we later factor in sentence length as a feature\n",
    "        sent_scores.append(score / len(sent_tokens))\n",
    "\n",
    "    return sent_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W3 Posisi Kalimat\n",
    "def position_score(self, i, size):\n",
    "        \"\"\" Yields a value between (0,1), corresponding to sentence's position in the article.\n",
    "            Assuming that sentences at the very beginning and ends of the article have a higher weight. \n",
    "            Values borrowed from https://github.com/xiaoxu193/PyTeaser\n",
    "        \"\"\"\n",
    "\n",
    "        relative_position = i / size\n",
    "        if 0 < relative_position <= 0.1:\n",
    "            return 0.17\n",
    "        elif 0.1 < relative_position <= 0.2:\n",
    "            return 0.23\n",
    "        elif 0.2 < relative_position <= 0.3:\n",
    "            return 0.14\n",
    "        elif 0.3 < relative_position <= 0.4:\n",
    "            return 0.08\n",
    "        elif 0.4 < relative_position <= 0.5:\n",
    "            return 0.05\n",
    "        elif 0.5 < relative_position <= 0.6:\n",
    "            return 0.04\n",
    "        elif 0.6 < relative_position <= 0.7:\n",
    "            return 0.06\n",
    "        elif 0.7 < relative_position <= 0.8:\n",
    "            return 0.04\n",
    "        elif 0.8 < relative_position <= 0.9:\n",
    "            return 0.04\n",
    "        elif 0.9 < relative_position <= 1.0:\n",
    "            return 0.15\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W4 Kemiripan dengan Judul\n",
    "def headline_score(self, headline, sentence):\n",
    "    \"\"\" Gives sentence a score between (0,1) based on percentage of words common to the headline. \"\"\"\n",
    "    title_stems = [stemmer.stem(w) for w in headline if w not in stop_words]\n",
    "    sentence_stems = [stemmer.stem(w) for w in sentence if w not in stop_words]\n",
    "    count = 0.0\n",
    "    for word in sentence_stems:\n",
    "        if word in title_stems:\n",
    "            count += 1.0\n",
    "    score = count / len(title_stems)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W5 Kemiripan dengan Trending Issue\n",
    "def simmilarity_trending_issue_score(self, headline, sentence):\n",
    "    \"\"\" Gives sentence a score between (0,1) based on percentage of words common to the headline. \"\"\"\n",
    "    title_stems = [stemmer.stem(w) for w in headline if w not in stop_words]\n",
    "    sentence_stems = [stemmer.stem(w) for w in sentence if w not in stop_words]\n",
    "    count = 0.0\n",
    "    for word in sentence_stems:\n",
    "        if word in title_stems:\n",
    "            count += 1.0\n",
    "    score = count / len(title_stems)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W6 mengandung entitas bernama\n",
    "def entity_score(self, headline, sentence):\n",
    "    \"\"\" Gives sentence a score between (0,1) based on percentage of words common to the headline. \"\"\"\n",
    "    title_stems = [stemmer.stem(w) for w in headline if w not in stop_words]\n",
    "    sentence_stems = [stemmer.stem(w) for w in sentence if w not in stop_words]\n",
    "    count = 0.0\n",
    "    for word in sentence_stems:\n",
    "        if word in title_stems:\n",
    "            count += 1.0\n",
    "    score = count / len(title_stems)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W7 Panjang kalimat\n",
    "def length_score(self, sentence):\n",
    "        \"\"\" Gives sentence score between (0,1) based on how close sentence's length is to the ideal length.\"\"\"\n",
    "        len_diff = math.fabs(ideal_sent_length - len(sentence))\n",
    "        return len_diff / ideal_sent_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WTot \n",
    "def score(article):\n",
    "        \"\"\" Assigns each sentence in the document a score based on the sum of features values.\n",
    "            Based on 4 features: relevance to headline, length, sentence position, and TF*IDF frequency.\n",
    "        \"\"\"\n",
    "\n",
    "        headline = article[0]\n",
    "        sentences = self.split_into_sentences(article[1])\n",
    "        frequency_scores = self.frequency_scores(article[1])\n",
    "\n",
    "        for i, s in enumerate(sentences):\n",
    "            headline_score = self.headline_score(headline, s) * 1.5\n",
    "            length_score = self.length_score(self.split_into_words(s)) * 1.0\n",
    "            position_score = self.position_score(float(i+1), len(sentences)) * 1.0\n",
    "            frequency_score = frequency_scores[i] * 4\n",
    "            score = (headline_score + frequency_score + length_score + position_score) / 4.0\n",
    "            self._scores[s] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PENYUSUNAN RINGKASAN BERITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengurutkan berat berita\n",
    "# menentukan jumlah kalimat hasil ringkasan\n",
    "def generate_summaries(self):\n",
    "    \"\"\" If article is shorter than the desired summary, just return the original articles.\"\"\"\n",
    "\n",
    "    # Rare edge case (when total num sentences across all articles is smaller than desired summary length)\n",
    "    total_num_sentences = 0\n",
    "    for article in self._articles:\n",
    "        total_num_sentences += len(self.split_into_sentences(article[1]))\n",
    "\n",
    "    if total_num_sentences <= SUMMARY_LENGTH:\n",
    "        return [x[1] for x in self._articles]\n",
    "\n",
    "    self.build_TFIDF_model()  # only needs to be done once\n",
    "\n",
    "    self._scores = Counter()\n",
    "    for article in self._articles:\n",
    "        self.score(article)\n",
    "\n",
    "    highest_scoring = self._scores.most_common(SUMMARY_LENGTH)\n",
    "    if DEBUG:\n",
    "        print(highest_scoring)\n",
    "\n",
    "    print(\"## Headlines: \")\n",
    "    for article in self._articles:\n",
    "        print(\"- \" + article[0])\n",
    "\n",
    "    # Appends highest scoring \"representative\" sentences, returns as a single summary paragraph.\n",
    "    return ' '.join([sent[0] for sent in highest_scoring])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_CLUSTER):\n",
    "    cluster_tf, cluster_terms =\n",
    "    get_tf(non_trivial[cluster_id == i].tolist(),get_terms = True)\n",
    "    cluster_tfidf = get_tfidf(non_trivial[cluster_id == i].tolist(),normalize = True)\n",
    "    cluster_wf = np.array(get_wf(non_trivial[cluster_id == i].tolist()).values())\n",
    "    selected = np.argwhere(np.logical_and(np.logical_and ((cluster_tfidf.max(axis = 0) > TFIDF), cluster_wf > WF), cluster_tf.max(axis = 0) > TF)).T[0]\n",
    "    keywords.extend([cluster_terms[idx] for idx in selected])self.result_dict['result'][i]['selected_terms']= [cluster_terms[idx] for idx in selected]\n",
    "    if len(self.result_dict['result'][i]['selected_terms']) > t_max:\n",
    "        t_max = len(self.result_dict['result'][i]['selected_terms'])\n",
    "        best_cluster = i\n",
    "    self.result_dict['result'][i]['members'] = cluster_tf.shape[0]\n",
    "self.result_dict['result'][i]['cluster_terms'] = cluster_terms\n",
    "max_score = 0\n",
    "for i in range(N_CLUSTER):\n",
    "    term_clust = []\n",
    "    for trm in non_trivial[cluster_id == i]:\n",
    "        term_clust.extend(trm)\n",
    "    score = 0\n",
    "    for keyword in keywords:\n",
    "        cou = term_clust.count(keyword)\n",
    "        if cou > CI_THRES:\n",
    "            score += np.log(term_clust.count(keyword) + 1)\n",
    "        if max_score < score:\n",
    "            max_score = score\n",
    "            best_cluster = i\n",
    "            self.result_dict['result'][i]['score'] = core\n",
    "self.selected_terms_final = self.result_dict['result'][best_cluster]['select ed_terms']                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UJI COBA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
